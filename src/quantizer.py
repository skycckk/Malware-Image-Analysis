import numpy as np
from scipy.cluster.vq import vq, kmeans, whiten
from scipy.spatial.distance import cdist
import os

working_path = os.path.dirname(os.path.abspath(__file__))
feature_folder_name = 'extracted_feats'
saved_feature_path = working_path + '/../' + feature_folder_name


def cluster_all_features(feature_mat):
    """
    Run k-means to cluster the input feature vectors
    :param feature_mat: m-by-n ndarray
            M is the number of samples and N is dimensionality
    :return: dictionary<k, (distortion, centroids)>
            This dictionary tells the distortion with what centroids and what's K
            key: k value
            Value: tuple with <distortion, centroids> where centroids are k-by-n ndarray
    """
    n_dims = feature_mat.shape[1]
    whitened = whiten(feature_mat.transpose())
    all_codebooks = dict()
    for k in range(n_dims, 0, -1):
        centroids, distortion = kmeans(whitened, k)
        all_codebooks[k] = (distortion, centroids)

    return all_codebooks


def cluster_feature(feature_mat, k):
    """
    Apply K-means to get the clusters' centroid and distortion
    :param feature_mat: m-by-n ndarray
            M is the number of samples and N is dimensionality
    :param k: int
            Number of centroids
    :return: <centroids, distortions>
            centroids: k-by-n ndarray
            distortion: overall distortion for k centroids
    """
    whitened = whiten(feature_mat.transpose())
    centroid, distortion = kmeans(whitened, k)

    return centroid, distortion


def mask_features(feat_file_name):
    """
    Mask the original feature vector to reduced features with the mask generated from SVM-RFE
    :param feat_file_name: str
            File name of saved feature.
    :return: m-by-n ndarray
            Reduced feature matrix.
            M is the number of samples and N is the reduced length.
    """
    with open(saved_feature_path + '/f1_reduced_mask', 'rb') as fp:
        mask = pickle.load(fp)

    with open(saved_feature_path + '/' + feat_file_name, 'rb') as fp:
        features = pickle.load(fp)

    feature_mat = list()
    for i in range(len(features)):
        feature_vec = list()
        for j in range(len(mask)):
            if mask[j]:
                feature_vec.append(features[i][j])
        feature_mat.append(feature_vec)

    feature_mat = np.asarray(feature_mat)
    return feature_mat


def train_threshold(ref_centroids, feature_mat, k):
    """
    Compute a threshold of a malware by using k-means distortion and its referenced codebook.
    :param ref_centroids: m-by-n ndarray
            The centroids of referenced sample.
    :param feature_mat: m-by-n ndarray
            M is the number of samples and N is dimensionality
    :return: float
            The threshold can distinguish it's family X or not.
    """
    all_dist = list()
    overall_max_dist = 0
    for i in range(len(feature_mat)):
        feature_vec = feature_mat[i]
        centroids_, distortion_ = cluster_feature(feature_vec, k)
        centroids_ = centroids_.reshape(len(centroids_), -1)

        # Compute distortion to decide the threshold
        #   scipy.cdist: Computes distance between each pair of the two collections of inputs
        #   Get the average minimum distance of each i to j pair where i is not equal to j
        dist = cdist(ref_centroids, centroids_, 'euclidean')
        code = dist.argmin(axis=1)
        min_dist_list = dist[np.arange(len(code)), code]
        avg_min_dist = min_dist_list.mean(axis=-1)
        all_dist.append(avg_min_dist)
        overall_max_dist = max(overall_max_dist, avg_min_dist)
    np_all_dist = np.asarray(all_dist)
    mean = np_all_dist.mean()
    std = np_all_dist.std()
    threshold = mean + 2 * std
    return threshold


    return overall_max_dist
