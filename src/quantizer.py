import numpy as np
from scipy.cluster.vq import vq, kmeans, whiten
from scipy.spatial.distance import cdist
import pickle
import os

working_path = os.path.dirname(os.path.abspath(__file__))
feature_folder_name = 'extracted_feats'
cluster_folder_name = 'clustered_res'
saved_feature_path = working_path + '/../' + feature_folder_name
saved_cluster_path = working_path + '/../' + cluster_folder_name
n_families = 25

def cluster_all_features(feature_mat):
    """
    Run k-means to cluster the input feature vectors
    :param feature_mat: m-by-n ndarray
            M is the number of samples and N is dimensionality
    :return: dictionary<k, (distortion, centroids)>
            This dictionary tells the distortion with what centroids and what's K
            key: k value
            Value: tuple with <distortion, centroids> where centroids are k-by-n ndarray
    """
    n_dims = feature_mat.shape[1]
    whitened = whiten(feature_mat.transpose())
    all_codebooks = dict()
    for k in range(n_dims, 0, -1):
        centroids, distortion = kmeans(whitened, k)
        all_codebooks[k] = (distortion, centroids)

    return all_codebooks


def cluster_feature(feature_mat, k):
    """
    Apply K-means to get the clusters' centroid and distortion
    :param feature_mat: m-by-n ndarray
            M is the number of samples and N is dimensionality
    :param k: int
            Number of centroids
    :return: <centroids, distortions>
            centroids: k-by-n ndarray
            distortion: overall distortion for k centroids
    """
    whitened = whiten(feature_mat.transpose())
    centroid, distortion = kmeans(whitened, k)

    return centroid, distortion


def mask_feature(feature):
    """
    Mask the original feature vector to reduced features with the mask generated from SVM-RFE
    :param feature: 1D List
            The original feature vector.
    :return: 1-by-d ndarray
            Reduced feature matrix.
    """
    with open(saved_feature_path + '/f1_reduced_mask', 'rb') as fp:
        mask = pickle.load(fp)

    feature_vec = list()
    for i in range(len(mask)):
        if mask[i]:
            feature_vec.append(feature[i])

    return np.asarray(feature_vec)


def mask_features(feat_file_name):
    """
    Mask the original feature vector to reduced features with the mask generated from SVM-RFE
    :param feat_file_name: str
            File name of saved feature.
    :return: m-by-n ndarray
            Reduced feature matrix.
            M is the number of samples and N is the reduced length.
    """
    with open(saved_feature_path + '/f1_reduced_mask', 'rb') as fp:
        mask = pickle.load(fp)

    with open(saved_feature_path + '/' + feat_file_name, 'rb') as fp:
        features = pickle.load(fp)

    feature_mat = list()
    for i in range(len(features)):
        feature_vec = list()
        for j in range(len(mask)):
            if mask[j]:
                feature_vec.append(features[i][j])
        feature_mat.append(feature_vec)

    feature_mat = np.asarray(feature_mat)
    return feature_mat


def train_threshold(ref_centroids, feature_mat, k):
    """
    Compute a threshold of a malware by using k-means distortion and its referenced codebook.
    :param ref_centroids: m-by-n ndarray
            The centroids of referenced sample.
    :param feature_mat: m-by-n ndarray
            M is the number of samples and N is dimensionality
    :return: float
            The threshold can distinguish it's family X or not.
    """
    all_dist = list()
    overall_max_dist = 0
    for i in range(len(feature_mat)):
        feature_vec = feature_mat[i]
        centroids_, distortion_ = cluster_feature(feature_vec, k)
        centroids_ = centroids_.reshape(len(centroids_), -1)

        # Compute distortion to decide the threshold
        #   scipy.cdist: Computes distance between each pair of the two collections of inputs
        #   Get the average minimum distance of each i to j pair where i is not equal to j
        dist = cdist(ref_centroids, centroids_, 'euclidean')
        code = dist.argmin(axis=1)
        min_dist_list = dist[np.arange(len(code)), code]
        avg_min_dist = min_dist_list.mean(axis=-1)
        all_dist.append(avg_min_dist)
        overall_max_dist = max(overall_max_dist, avg_min_dist)
    np_all_dist = np.asarray(all_dist)
    mean = np_all_dist.mean()
    std = np_all_dist.std()
    threshold = mean + 2 * std
    return threshold


def train_with_inner_outer(feature_mat, fam_index):
    """
    Train the K-means K value by evaluating inner and outer measurement.
    For each k value, compute training samples' centroids, determine a threshold,
    and then compute the accuracy for inner family and outer families. And then
    use the inner/outer accuracy to compute a score which will decide how well is the chosen k.
    :param feature_mat: m-by-n ndarray
            M is the number of samples and N is dimensionality
    :param fam_index: int
            The index of malware family.
    :return: dictionary
            A dictionary storing best_k, family_index, inner_rate, outer_rates
    """
    # collapse
    features = np.mean(feature_mat, axis=0).reshape((1, feature_mat.shape[1]))

    # get all possible centroids with different k from 1 to k
    codebooks = cluster_all_features(features)
    max_coeff_s = -1
    best_k = 0

    result_dict = dict()
    for k in codebooks.keys():
        print('k = ', k)
        distortion, centroids = codebooks[k]
        threshold = train_threshold(centroids, feature_mat, k)

        # start testing with other families
        inner_rate = 0
        outer_rates = list()
        for n in range(n_families):
            f1_fam_name = 'f1_fam_' + str(n) + '_features'
            feature_mat_outer = mask_features(f1_fam_name)
            n_samples = len(feature_mat_outer)

            # take 80% of samples as training
            feature_mat_outer = feature_mat_outer[int(n_samples / 5):, :]

            # compute accuracy
            hit = 0
            for outer_vec in feature_mat_outer:
                centroids_, distortion_ = cluster_feature(outer_vec, k)
                centroids_ = centroids_.reshape(len(centroids_), -1)
                dist = cdist(centroids, centroids_, 'euclidean')
                code = dist.argmin(axis=1)
                min_dist_list = dist[np.arange(len(code)), code]
                avg_min_dist = min_dist_list.mean(axis=-1)
                if avg_min_dist < threshold:
                    hit += 1

            accuracy = hit / len(feature_mat_outer) * 100
            if n == fam_index:
                inner_rate = accuracy
            else:
                outer_rates.append(accuracy)

        coeff_inner = inner_rate
        coeff_outer = np.median(outer_rates) if len(outer_rates) > 0 else 0
        coeff_s = 1 - coeff_outer / coeff_inner
        if coeff_s > max_coeff_s:
            max_coeff_s = coeff_s
            best_k = k
            result_dict['fam'] = fam_index
            result_dict['k'] = best_k
            result_dict['inner_rate'] = inner_rate
            result_dict['outer_rates'] = outer_rates

    return best_k, result_dict


def train_k():
    """
    An entrance for training value k in k-means.
    This will dump the result of trained k_value, family_index, and rates
    :return:
    """

    for i in range(n_families):
        print('[training-k] family:', i, '...')
        f1_fam_name = 'f1_fam_' + str(i) + '_features'
        feature_mat = mask_features(f1_fam_name)
        n_samples = len(feature_mat)
        # use 80% of samples as training set
        feature_mat = feature_mat[int(n_samples / 5):, :]
        my_k, my_result = train_with_inner_outer(feature_mat, i)

        save_name = '/f1_fam_' + str(i) + '_validation'
        with open(saved_cluster_path + save_name, 'wb') as fp:
            pickle.dump(my_result, fp)

