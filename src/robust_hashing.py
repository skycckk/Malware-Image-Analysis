from hamming_code import *
from extract_features import frequency_feature
from scipy.misc import imread
from numpy import uint16
from numpy import uint8
from numpy import set_printoptions
from numpy import asarray
from load_images import *
from numpy import mean
from numpy import vstack
from numpy import linalg
from analysis_plot import *
import pickle

set_printoptions(suppress=True)


class RobustHashing:
    def __init__(self):
        pass

    def hash_with_hamming_decode(self, img):
        """
        Hash an image with 1. extracting features 2. using Hamming decoder to decode
        :param img: An 2D one channel input image.
        :return: A list of decoded integers
        """
        feature_vec = frequency_feature(img)
        coder = HammingCode74()
        hash_val = list()
        for val in feature_vec:
            dword = uint16(val)
            dword_hi = uint8(dword >> 8)
            dword_lo = uint8(dword & 0x00ff)
            decoded_hi = coder.decode(dword_hi)
            decoded_lo = coder.decode(dword_lo)

            hash_val.append((decoded_hi << 8) | decoded_lo)

        return hash_val

    def hash_with_hamming_encode(self, img):
        """
        Hash an image by decoding and extracting its syndrome (3 bits in hamming (7, 4)
        :param img: An 2D one channel input image
        :return: A string with all syndrome bits
        """
        feature_vec = frequency_feature(img)
        coder = HammingCode74()
        syndrome = [0] * len(feature_vec) * 2
        hash_str = str()
        cnt = 0
        for val in feature_vec:
            dword = uint16(val)
            dword_hi = uint8(dword >> 8)
            dword_lo = uint8(dword & 0x00ff)

            syndrome_hi = coder.get_syndrome(dword_hi)
            syndrome_lo = coder.get_syndrome(dword_lo)
            syndrome[cnt] = str(syndrome_hi[0]) + str(syndrome_hi[1]) + str(syndrome_hi[2])
            syndrome[cnt + 1] = str(syndrome_lo[0]) + str(syndrome_lo[1]) + str(syndrome_lo[2])

            cnt += 2

        return hash_str.join(syndrome)

    def batch_hash(self):
        """
        Run a batch hashing process for a given image folder.
        It will write the result into a binary file in a folder.
        There are different binary files depending on the processed families.
        Each resulting binary file is a list of hashed value for all images in a family.
        Example. hash the images in /dataset and dump the result to /hashed_values/decode/
        :return:
        """
        dataset_path = '../dataset_test'
        image_list = prefetch_images(dataset_path)
        n_families = len(image_list)
        rbh = RobustHashing()

        for i in range(n_families):
            family_name = image_list[i][0]
            image_names = image_list[i][1]
            n_images = len(image_names)
            print('processing ' + str(i) + ' :' + str(n_images) + ' images...')

            hashed_mean = 0
            hashed_all = [0] * n_images
            for j in range(n_images):
                image_path = dataset_path + '/' + family_name + '/' + image_names[j]
                img = imread(image_path).astype(float)
                hashed = asarray(rbh.hash_with_hamming_decode(img))
                hashed_mean += hashed
                hashed_all[j] = hashed
            hashed_mean = hashed_mean / n_images

            variance = 0
            for j in range(n_images):
                variance += (hashed_all[i] - hashed_mean) ** 2
            variance = variance / n_images

            with open('../hashed_values/decode/' + family_name, 'wb') as fp:
                pickle.dump(hashed_all, fp)

    def training(self, family_name=None):
        """
        Train the hashed value with using Hamming decode to generate a representative key for a certain family.
        This process includes using 20% of samples then combine a mean value.
        :param family_name: Family name of None it will load all files(families) from /hashed_values/decode
        :return: A dictionary: <family name, dict<mean_vec, max_dist>> where mean_vec is the hash key.
        """
        if family_name is not None:
            all_files = [family_name]
        else:
            all_files = listdir('../hashed_values/decode/')

        keys_dict = dict()
        for family in all_files:
            with open('../hashed_values/decode/' + family, 'rb') as fp:
                all_hashed_values = pickle.load(fp)

            # pick 20% of samples as training set
            n_hashed = len(all_hashed_values)
            training = all_hashed_values[0:int(0.2 * n_hashed)]
            testing = all_hashed_values[int(0.2 * n_hashed) + 1:]
            mean_vec = mean(vstack(training), axis=0)
            max_dist = 0
            for each in training:
                max_dist = max(max_dist, linalg.norm(each - mean_vec))

            keys_pair = dict()
            keys_pair['mean_vec'] = mean_vec
            keys_pair['max_dist'] = max_dist
            keys_dict[family] = keys_pair

            positive = 0
            for each in testing:
                dist = linalg.norm(each - mean_vec)
                if dist <= max_dist:
                    positive += 1
            # This points out the inside-test accuracy
            # print('accuracy rate:', positive / len(testing), '(' + family + ')')
        return keys_dict

    def testing(self, keys_dict, family_name=None):
        """
        The testing process. Classify the family by using minimum euclidean distance, that is, classify I to Ii if
        distance(I, Ii) is the minimum distance for i = 1 ~ N.
        Basically, this method will generate a confusion matrix and write it to file.
        :param keys_dict: A key set from training().
        :param family_name: Family name of None if it will load all files(families) from /hashed_values/decode
        :return:
        """
        if family_name is not None:
            all_files = [family_name]
        else:
            all_files = listdir('../hashed_values/decode/')

        # prepare a confusion matrix
        # confusion_row_list: 2D list where each row is the classification result and column is the family
        # confusion_row_schema: 1D list indicating the family name with row index corresponding to confusion_row_list
        # confusion_col_schema: 1D list indicating the family name with column index corresponding to confusion_row_list
        confusion_row_list = list()
        confusion_row_schema = list()
        confusion_col_schema = list()
        for key in keys_dict:
            confusion_col_schema.append(key)

        for family in all_files:
            with open('../hashed_values/decode/' + family, 'rb') as fp:
                all_hashed_values = pickle.load(fp)

            # pick 80% of samples as training set
            n_hashed = len(all_hashed_values)
            testing = all_hashed_values[int(0.2 * n_hashed) + 1:]
            positive = 0
            accuracy_row = [0] * len(keys_dict)
            for each in testing:
                curr_idx = 0
                min_dist = 10e7
                min_dist_idx = 0
                for key in keys_dict:
                    keys_pair = keys_dict[key]
                    mean_vec = keys_pair['mean_vec']
                    max_dist = keys_pair['max_dist']

                    dist = linalg.norm(each - mean_vec)
                    # if dist <= max_dist:
                    #     min_dist = min(min_dist, dist)
                    #     min_dist_idx = curr_idx
                    if dist <= min_dist:
                        min_dist = dist
                        min_dist_idx = curr_idx

                    curr_idx += 1

                accuracy_row[min_dist_idx] += 1
                if family == confusion_col_schema[min_dist_idx]:
                    positive += 1

            confusion_row_list.append(accuracy_row)
            confusion_row_schema.append(family)

            print('Overall accuracy for ' + family + ' : ',  positive / len(testing))

        with open('../hashed_values/decode_result/cm_row', 'wb') as fp:
            pickle.dump(confusion_row_list, fp)
        with open('../hashed_values/decode_result/cm_row_schema', 'wb') as fp:
            pickle.dump(confusion_row_schema, fp)
        with open('../hashed_values/decode_result/cm_col_schema', 'wb') as fp:
            pickle.dump(confusion_col_schema, fp)


def produce_accuracy():
    with open('../hashed_values/decode_result/cm_row', 'rb') as fp:
        cm_row_list = pickle.load(fp)
    with open('../hashed_values/decode_result/cm_row_schema', 'rb') as fp:
        cm_row_schema = pickle.load(fp)
    with open('../hashed_values/decode_result/cm_col_schema', 'rb') as fp:
        cm_col_schema = pickle.load(fp)

    n_families = len(cm_row_schema)
    accuracy_list = [0] * n_families
    accuracy_name = [0] * n_families
    accuracy_tuple = [0] * n_families
    for i in range(n_families):
        family = cm_row_schema[i]
        cm = cm_row_list[i]
        col_idx = cm_col_schema.index(family)

        accuracy_list[i] = cm[col_idx] / sum(cm)
        accuracy_name[i] = family
        accuracy_tuple[i] = (accuracy_list[i], accuracy_name[i])
    avg_accuracy = mean(accuracy_list) * 100

    # sort the accuracy list from highest to lowest
    accuracy_tuple.sort(key=lambda tup: tup[0], reverse=True)

    for i in range(n_families):
        accuracy_list[i] = accuracy_tuple[i][0] * 100
        accuracy_name[i] = accuracy_tuple[i][1]

    # plot classification result
    fig = plot_curve(accuracy_list, xtick_names=accuracy_name, xtick_rotation_deg=90,
                     xlabel='Family name', ylabel='Accuracy',
                     title='Classification with Hamming decode (Avg. = ' + str('%0.2f' % avg_accuracy) + '%)',
                     grid=True, show=False)

    # plot extra information:
    #   1. separate different tiers for each accuracy
    #   2. plot an accumulated accuracy
    accu_accuracy = [0] * n_families
    accu_sum, accu_cnt = 0, 0
    for i in range(n_families):
        accu_sum += accuracy_list[i]
        accu_cnt += 1
        accu_accuracy[i] = accu_sum / accu_cnt

    tier1 = [i for i in range(len(accuracy_list)) if 90 <= accuracy_list[i]]
    tier2 = [i for i in range(len(accuracy_list)) if 70 <= accuracy_list[i] < 90]
    tier3 = [i for i in range(len(accuracy_list)) if accuracy_list[i] < 70]

    tier1_accuracy = [accuracy_list[idx] for idx in tier1]
    tier2_accuracy = [accuracy_list[idx] for idx in tier2]
    tier3_accuracy = [accuracy_list[idx] for idx in tier3]

    ax = fig.gca()
    ax.get_lines()[0].set_label('_nolegend_')
    ax.scatter(tier1, tier1_accuracy, marker='o', c='red', zorder=10)
    ax.scatter(tier2, tier2_accuracy, marker='^', c='green', zorder=10)
    ax.scatter(tier3, tier3_accuracy, marker='*', c='gray', zorder=10)
    ax.plot(accu_accuracy)
    ax.legend(('Moving average',
               'Tier1(90 ≤ accu): ' + str('%.2f' % mean(tier1_accuracy)) + '%',
               'Tier2(70 ≤ accu < 90): ' + str('%.2f' % mean(tier2_accuracy)) + '%',
               'Tier3(accu < 70): ' + str('%.2f' % mean(tier3_accuracy)) + '%'))

    fig.show()


def test():
    print('Example:')
    img = imread('../dataset/Adialer.C/00bb6b6a7be5402fcfce453630bfff19.png').astype(float)
    rbh = RobustHashing()
    hashed = rbh.hash_with_hamming_decode(img)
    print(hashed)

    hashed = rbh.hash_with_hamming_encode(img)
    print(hashed)
    print('Example: ends')

    rbh.batch_hash()

    keys_dict = rbh.training()
    rbh.testing(keys_dict)

    produce_accuracy()


if __name__ == '__main__':
    test()
