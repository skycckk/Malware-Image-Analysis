from hamming_code import *
from extract_features import frequency_feature
from scipy.misc import imread
from numpy import uint16
from numpy import uint8
from numpy import set_printoptions
from numpy import asarray
from load_images import *
import pickle


class RobustHashing:
    def __init__(self):
        pass

    def hash_with_hamming_decode(self, img):
        """
        Hash an image with 1. extracting features 2. using Hamming decoder to decode
        :param img: An 2D one channel input image.
        :return: A list of decoded integers
        """
        feature_vec = frequency_feature(img)
        coder = HammingCode74()
        hash_val = list()
        for val in feature_vec:
            dword = uint16(val)
            dword_hi = uint8(dword >> 8)
            dword_lo = uint8(dword & 0x00ff)
            decoded_hi = coder.decode(dword_hi)
            decoded_lo = coder.decode(dword_lo)

            hash_val.append((decoded_hi << 8) | decoded_lo)

        return hash_val

    def hash_with_hamming_encode(self, img):
        """
        Hash an image by decoding and extracting its syndrome (3 bits in hamming (7, 4)
        :param img: An 2D one channel input image
        :return: A string with all syndrome bits
        """
        feature_vec = frequency_feature(img)
        coder = HammingCode74()
        syndrome = [0] * len(feature_vec) * 2
        hash_str = str()
        cnt = 0
        for val in feature_vec:
            dword = uint16(val)
            dword_hi = uint8(dword >> 8)
            dword_lo = uint8(dword & 0x00ff)

            syndrome_hi = coder.get_syndrome(dword_hi)
            syndrome_lo = coder.get_syndrome(dword_lo)
            syndrome[cnt] = str(syndrome_hi[0]) + str(syndrome_hi[1]) + str(syndrome_hi[2])
            syndrome[cnt + 1] = str(syndrome_lo[0]) + str(syndrome_lo[1]) + str(syndrome_lo[2])

            cnt += 2

        return hash_str.join(syndrome)

    def batch_hash(self):
        """
        Run a batch hashing process for a given image folder.
        It will write the result into a binary file in a folder.
        There are different binary files depending on the processed families.
        Each resulting binary file is a list of hashed value for all images in a family.
        Example. hash the images in /dataset and dump the result to /hashed_values/decode/
        :return:
        """
        dataset_path = '../dataset_test'
        image_list = prefetch_images(dataset_path)
        n_families = len(image_list)
        rbh = RobustHashing()

        for i in range(n_families):
            family_name = image_list[i][0]
            image_names = image_list[i][1]
            n_images = len(image_names)
            print('processing ' + str(i) + ' :' + str(n_images) + ' images...')

            hashed_mean = 0
            hashed_all = [0] * n_images
            for j in range(n_images):
                image_path = dataset_path + '/' + family_name + '/' + image_names[j]
                img = imread(image_path).astype(float)
                hashed = asarray(rbh.hash_with_hamming_decode(img))
                hashed_mean += hashed
                hashed_all[j] = hashed
            hashed_mean = hashed_mean / n_images

            variance = 0
            for j in range(n_images):
                variance += (hashed_all[i] - hashed_mean) ** 2
            variance = variance / n_images

            with open('../hashed_values/decode/' + family_name, 'wb') as fp:
                pickle.dump(hashed_all, fp)


def test():
    print('Example:')
    img = imread('../dataset/Adialer.C/00bb6b6a7be5402fcfce453630bfff19.png').astype(float)
    rbh = RobustHashing()
    hashed = rbh.hash_with_hamming_decode(img)
    print(hashed)

    hashed = rbh.hash_with_hamming_encode(img)
    print(hashed)
    print('Example: ends')

    rbh.batch_hash()


if __name__ == '__main__':
    test()
