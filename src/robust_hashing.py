from hamming_code import *
from extract_features import frequency_feature
from extract_features import wavelet_features
from scipy.misc import imread
from numpy import uint16
from numpy import uint8
from numpy import set_printoptions
from numpy import asarray
from load_images import *
from numpy import mean
from numpy import vstack
from numpy import linalg
from numpy import arange
from analysis_plot import *
import pickle
import random

set_printoptions(suppress=True)


def load_feature_files(path):
    all_files = listdir(path)
    black_list = ['README.md', '.DS_Store']
    for item in black_list:
        if item in all_files:
            all_files.remove(item)
    return all_files


class RobustHashing:
    # 0: using hamming decode directly.
    # 1: using the syndrome (encode)
    hashing_method = 0
    output_path = ''
    output_testing_path = ''

    def __init__(self, hashing_method=0):
        self.hashing_method = hashing_method
        if hashing_method == 0:
            self.output_path = '../hashed_values/decode/'
            self.output_testing_path = '../hashed_values/decode_result/'
            print('Hashing method: Hamming decoding')
        elif hashing_method == 1:
            self.output_path = '../hashed_values/encode/'
            self.output_testing_path = '../hashed_values/encode_result/'
            print('Hashing method: Hamming encoding (syndrome only)')
        else:
            raise Exception('UN-SUPPORT HASHING METHOD!')

    def hash_with_hamming_decode(self, img):
        """
        Hash an image with 1. extracting features 2. using Hamming decoder to decode
        :param img: An 2D one channel input image.
        :return: A list of decoded integers
        """
        feature_vec = frequency_feature(img)
        coder = HammingCode74()
        hash_val = [0] * len(feature_vec)
        for i in range(len(feature_vec)):
            val = feature_vec[i]
            dword = uint16(val)
            dword_hi = uint8(dword >> 8)
            dword_lo = uint8(dword & 0x00ff)
            decoded_hi = coder.decode(dword_hi)
            decoded_lo = coder.decode(dword_lo)

            hash_val[i] = (decoded_hi << 8) | decoded_lo

        return hash_val

    def hash_with_hamming_decode_byte(self, img):
        """
        Same as hash_with_hamming_decode() except for this one is uing one byte as the input
        :param img: An 2D one channel input image.
        :return: A list of decoded integers
        """
        coder = HammingCode74()
        feature_vec = wavelet_features(img)
        hash_val = [0] * len(feature_vec)
        for i in range(len(feature_vec)):
            val = feature_vec[i]
            decoded = coder.decode(val)
            hash_val[i] = decoded

        return hash_val

    def hash_with_hamming_encode(self, img):
        """
        Hash an image by decoding and extracting its syndrome (3 bits in hamming (7, 4)
        :param img: An 2D one channel input image
        :return: A string with all syndrome bits
        """
        feature_vec = frequency_feature(img, line_bk=8)
        coder = HammingCode74()
        syndrome = [0] * len(feature_vec) * 2
        hash_str = str()
        cnt = 0
        for val in feature_vec:
            dword = uint16(val)
            dword_hi = uint8(dword >> 8)
            dword_lo = uint8(dword & 0x00ff)

            syndrome_hi = coder.get_syndrome(dword_hi)
            syndrome_lo = coder.get_syndrome(dword_lo)
            syndrome[cnt] = str(syndrome_hi[0]) + str(syndrome_hi[1]) + str(syndrome_hi[2])
            syndrome[cnt + 1] = str(syndrome_lo[0]) + str(syndrome_lo[1]) + str(syndrome_lo[2])

            cnt += 2

        return hash_str.join(syndrome)

    def batch_hash(self):
        """
        Run a batch hashing process for a given image folder.
        It will write the result into a binary file in a folder.
        There are different binary files depending on the processed families.
        Each resulting binary file is a list of hashed value for all images in a family.
        Example. hash the images in /dataset and dump the result to self.output_path. e.g. /hashed_values/decode/
        :return:
        """
        dataset_path = '../dataset'
        image_list = prefetch_images(dataset_path)
        n_families = len(image_list)

        trim_dataset = True
        n_picked = 200
        random.seed(1)
        for i in range(n_families):
            family_name = image_list[i][0]
            image_names = image_list[i][1]
            n_images = len(image_names)

            if trim_dataset and n_images > n_picked:
                picked_index = random.choices(list(arange(n_images)), k=n_picked)
                n_images = n_picked
            else:
                picked_index = arange(n_images)

            print('processing ' + str(i) + ' :' + str(n_images) + ' images...')

            hashed_all = [0] * n_images
            for j in range(n_images):
                img_index = picked_index[j]
                image_path = dataset_path + '/' + family_name + '/' + image_names[img_index]
                img = imread(image_path).astype(float)
                hashed = None
                if self.hashing_method is 0:  # decoding with Hamming74
                    hashed = asarray(self.hash_with_hamming_decode(img))
                elif self.hashing_method is 1:
                    hashed = self.hash_with_hamming_encode(img)
                else:
                    assert True, "NOT SUPPORT HASHING METHOD"
                hashed_all[j] = hashed

            with open(self.output_path + family_name, 'wb') as fp:
                pickle.dump(hashed_all, fp)

    def training(self, family_name=None, n_fold=5, cv_index=0):
        """
        Train the hashed value with the hashing method to generate a representative key for a certain family.
        This process includes using 20% of samples then combine a mean value.
        :param family_name: Family name of None it will load all files(families) from self.output_path
        :param n_fold: Number of cross-validation will perform.
        :param cv_index: At what round of performing cross-validation.
        :return: A dictionary: <family name, dict<mean_vec, max_dist>> where mean_vec is the hash key.
        """
        if family_name is not None:
            all_files = [family_name]
        else:
            all_files = load_feature_files(self.output_path)

        keys_dict = dict()
        for family in all_files:
            with open(self.output_path + family, 'rb') as fp:
                all_hashed_values = pickle.load(fp)

            # pick 20% of samples as training set
            n_hashed = len(all_hashed_values)
            training_size_factor = 1 / n_fold
            training_size = int(training_size_factor * n_hashed)
            start_index = int(cv_index * training_size)
            end_index = int(start_index + training_size)

            training = all_hashed_values[start_index:end_index]

            if self.hashing_method is 0:
                mean_vec = mean(vstack(training), axis=0)
                max_dist = 0
                for each in training:
                    max_dist = max(max_dist, linalg.norm(each - mean_vec))

                keys_pair = dict()
                keys_pair['mean_vec'] = mean_vec
                keys_pair['max_dist'] = max_dist
                keys_dict[family] = keys_pair
            elif self.hashing_method is 1:
                vote = [{'0': 0, '1': 0} for i in range(len(training[0]))]
                for i in range(len(training)):
                    each = training[i]
                    for j in range(len(each)):
                        bit = each[j]
                        vote[j][bit] += 1

                mean_hashed = ''
                for bucket in vote:
                    if bucket['0'] > bucket['1']:
                        mean_hashed += '0'
                    else:
                        mean_hashed += '1'

                keys_pair = dict()
                keys_pair['mean_vec'] = mean_hashed
                keys_dict[family] = keys_pair
            else:
                raise Exception('UN-SUPPORTED HASHING METHOD')

        return keys_dict

    def testing(self, keys_dict, family_name=None, n_fold=5, cv_index=0):
        """
        The testing process. Classify the family by using minimum euclidean distance if using decode method, that is,
        classify I to Ii if distance(I, Ii) is the minimum distance for i = 1 ~ N.
        If hashing method is encoding, then compare a minimum hamming distance.
        Basically, this method will generate a confusion matrix and write it to file.
        :param keys_dict: A key set from training().
        :param family_name: Family name of None if it will load all files(families) from self.output_path
        :param n_fold: Number of cross-validation will perform.
        :param cv_index: At what round of performing cross-validation.
        :return:
        """
        if family_name is not None:
            all_files = [family_name]
        else:
            all_files = load_feature_files(self.output_path)

        # prepare a confusion matrix
        # confusion_row_list: 2D list where each row is the classification result and column is the family
        # confusion_row_schema: 1D list indicating the family name with row index corresponding to confusion_row_list
        # confusion_col_schema: 1D list indicating the family name with column index corresponding to confusion_row_list
        confusion_row_list = list()
        confusion_row_schema = list()
        confusion_col_schema = list()
        for key in keys_dict:
            confusion_col_schema.append(key)

        for family in all_files:
            with open(self.output_path + family, 'rb') as fp:
                all_hashed_values = pickle.load(fp)

            # pick 80% of samples as training set
            n_hashed = len(all_hashed_values)
            training_size_factor = 1 / n_fold
            training_size = int(training_size_factor * n_hashed)
            start_index = int(cv_index * training_size)
            end_index = int(start_index + training_size)
            testing = all_hashed_values[:start_index] + all_hashed_values[end_index:]
            positive = 0
            accuracy_row = [0] * len(keys_dict)
            for each in testing:
                curr_idx = 0
                min_dist = 10e7
                min_dist_idx = 0
                for key in keys_dict:
                    keys_pair = keys_dict[key]

                    if self.hashing_method is 0:
                        mean_vec = keys_pair['mean_vec']
                        dist = linalg.norm(each - mean_vec)
                        if dist <= min_dist:
                            min_dist = dist
                            min_dist_idx = curr_idx
                        curr_idx += 1
                    elif self.hashing_method is 1:
                        mean_vec = keys_pair['mean_vec']  # mean_vec is a string with bits
                        dist = HammingCode74.hamming_distance(each, mean_vec)
                        if dist <= min_dist:
                            min_dist = dist
                            min_dist_idx = curr_idx
                        curr_idx += 1
                    else:
                        raise Exception('UN_SUPPORTED HASHING METHOD')

                accuracy_row[min_dist_idx] += 1
                if family == confusion_col_schema[min_dist_idx]:
                    positive += 1

            confusion_row_list.append(accuracy_row)
            confusion_row_schema.append(family)

            print('Overall accuracy for ' + family + ' : ',  positive / len(testing))

        with open(self.output_testing_path + 'cm_row', 'wb') as fp:
            pickle.dump(confusion_row_list, fp)
        with open(self.output_testing_path + 'cm_row_schema', 'wb') as fp:
            pickle.dump(confusion_row_schema, fp)
        with open(self.output_testing_path + 'cm_col_schema', 'wb') as fp:
            pickle.dump(confusion_col_schema, fp)

    def produce_accuracy(self):
        with open(self.output_testing_path + 'cm_row', 'rb') as fp:
            cm_row_list = pickle.load(fp)
        with open(self.output_testing_path + 'cm_row_schema', 'rb') as fp:
            cm_row_schema = pickle.load(fp)
        with open(self.output_testing_path + 'cm_col_schema', 'rb') as fp:
            cm_col_schema = pickle.load(fp)

        n_families = len(cm_row_schema)
        accuracy_list = [0] * n_families
        accuracy_name = [0] * n_families
        accuracy_tuple = [0] * n_families
        pos_cnt = 0
        pos_neg_cnt = 0
        for i in range(n_families):
            family = cm_row_schema[i]
            cm = cm_row_list[i]
            col_idx = cm_col_schema.index(family)

            accuracy_list[i] = cm[col_idx] / sum(cm)
            accuracy_name[i] = family
            accuracy_tuple[i] = (accuracy_list[i], accuracy_name[i])

            pos_cnt += cm[col_idx]
            pos_neg_cnt += sum(cm)
        avg_accuracy = mean(accuracy_list) * 100
        avg_accuracy_for_samples = pos_cnt / pos_neg_cnt * 100

        # sort the accuracy list from highest to lowest
        accuracy_tuple.sort(key=lambda tup: tup[0], reverse=True)

        for i in range(n_families):
            accuracy_list[i] = accuracy_tuple[i][0] * 100
            accuracy_name[i] = accuracy_tuple[i][1]

        # plot classification result
        fig = plot_curve(accuracy_list, xtick_names=accuracy_name, xtick_rotation_deg=90,
                         xlabel='Family name', ylabel='Accuracy',
                         title='Classification with Hamming decode.\n' +
                               'Avg. = ' + str('%.2f' % avg_accuracy) + '% (families)\n ' +
                               'Avg. = ' + str('%.2f' % avg_accuracy_for_samples) + '% (samples)',
                         grid=True, show=False)

        # plot extra information:
        #   1. separate different tiers for each accuracy
        #   2. plot an accumulated accuracy
        accu_accuracy = [0] * n_families
        accu_sum, accu_cnt = 0, 0
        for i in range(n_families):
            accu_sum += accuracy_list[i]
            accu_cnt += 1
            accu_accuracy[i] = accu_sum / accu_cnt

        tier1 = [i for i in range(len(accuracy_list)) if 90 <= accuracy_list[i]]
        tier2 = [i for i in range(len(accuracy_list)) if 70 <= accuracy_list[i] < 90]
        tier3 = [i for i in range(len(accuracy_list)) if accuracy_list[i] < 70]

        tier1_accuracy = [accuracy_list[idx] for idx in tier1]
        tier2_accuracy = [accuracy_list[idx] for idx in tier2]
        tier3_accuracy = [accuracy_list[idx] for idx in tier3]

        ax = fig.gca()
        ax.set_ylim([0, 100])
        ax.get_lines()[0].set_label('_nolegend_')
        ax.scatter(tier1, tier1_accuracy, marker='o', c='red', zorder=10)
        ax.scatter(tier2, tier2_accuracy, marker='^', c='green', zorder=10)
        ax.scatter(tier3, tier3_accuracy, marker='*', c='gray', zorder=10)
        ax.plot(accu_accuracy)
        ax.legend(('Moving average',
                   'Tier1(90 ≤ accu): ' + str('%.2f' % mean(tier1_accuracy)) + '%',
                   'Tier2(70 ≤ accu < 90): ' + str('%.2f' % mean(tier2_accuracy)) + '%',
                   'Tier3(accu < 70): ' + str('%.2f' % mean(tier3_accuracy)) + '%'))

        fig.show()

        return avg_accuracy, avg_accuracy_for_samples


def test():
    # print('Example:')
    # img = imread('../dataset/Adialer.C/00bb6b6a7be5402fcfce453630bfff19.png').astype(float)
    rbh = RobustHashing(hashing_method=0)
    # hashed = rbh.hash_with_hamming_decode(img)
    # print(hashed)
    #
    # hashed = rbh.hash_with_hamming_encode(img)
    # print(hashed)
    # print('Example: ends')
    #
    rbh.batch_hash()


    accuracy = 0
    n_fold = 5
    for i in range(n_fold):
        keys_dict = rbh.training(n_fold=n_fold, cv_index=i)
        rbh.testing(keys_dict, n_fold=n_fold, cv_index=i)
        avg_accuracy, dummy = rbh.produce_accuracy()
        accuracy += avg_accuracy
    print(str(n_fold) + '-fold accuracy: ', accuracy / n_fold)


if __name__ == '__main__':
    test()
