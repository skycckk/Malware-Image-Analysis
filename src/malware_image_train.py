from load_images import *
from extract_features import *
from skimage import transform
from sklearn import svm
from sklearn.feature_selection import RFE
from quantizer import *
import random


def get_reduced_features(features, n_dims_RFE=1):
    """
    Get reduced feature vectors by using SVM-RFE with specific no. dimensions
    :param features:
    :param n_dims_RFE:
    :return:
    """
    training_set = list()
    training_labels = list()
    for family in features:
        feature_mat = features.get(family)
        for j in range(len(feature_mat)):
            training_set.append(feature_mat[j])
            training_labels.append(family)

    clf = svm.SVC(kernel='linear')
    clf_reduced = RFE(clf, n_dims_RFE, step=1)
    clf_reduced = clf_reduced.fit(training_set, training_labels)
    X_new = clf_reduced.transform(training_set)
    X_mask = clf_reduced.get_support()
    return X_new, X_mask


def fitting_scoring(features, cv=5, verbose=False, is_RFE_mode=False, n_dims_RFE=1):
    """
    Train SVM model and score it.
    :param features: dict{family name: feature matrix}
                    feature_matrix is MxN where M is number of samples
                    and N is the dimension of a feature sample
    :param cv: int (optional)
            Number of cross-validation. Default is 5-fold
    :param verbose: boolean (optional)
            Flag to dump necessary message
    :param is_RFE_mode: boolean (optional)
            If it set to True, then this funciton will train by using linear-SVM with reduced size.
            If set to False, the RBF kernel will be used.
    :param n_dims_RFE: int (optional)
            Number of reduced dimension is used in SVM-RFE
    :return:
            Return a average accuracy over cross-validation.
            If is_RFE_mode is set, extra parameters are return.
                X_new: reduced feature vectors.
                X_mask: boolean mask showing which dim is used or not
    """
    # N-fold cross-validation
    num_fold = cv
    accuracy = [0] * num_fold
    for i in range(num_fold):
        training_set = list()
        training_labels = list()
        testing_set = list()
        testing_labels = list()
        for family in features:
            feature_mat = features.get(family)
            if verbose: print(family, "sample size:", len(feature_mat))

            fold_start = i * int(len(feature_mat) / num_fold)
            fold_end = fold_start + int(len(feature_mat) / num_fold) - 1

            # separate training and testing set
            for j in range(len(feature_mat)):
                if fold_start <= j <= fold_end:
                    testing_set.append(feature_mat[j])
                    testing_labels.append(family)
                else:
                    training_set.append(feature_mat[j])
                    training_labels.append(family)

        p_res = None
        X_new = None
        X_mask = None
        if is_RFE_mode:
            clf = svm.SVC(kernel='linear')
            clf_reduced = RFE(clf, n_dims_RFE, step=1)
            clf_reduced = clf_reduced.fit(training_set, training_labels)
            X_new = clf_reduced.transform(training_set)
            X_mask = clf_reduced.get_support()
            p_res = clf_reduced.predict(testing_set)
        else:
            clf = svm.SVC()
            clf.fit(training_set, training_labels)
            p_res = clf.predict(testing_set)

        accuracy[i] = 0
        for j in range(len(p_res)):
            if p_res[j] == testing_labels[j]:
                accuracy[i] += 1
        accuracy[i] = (accuracy[i] / len(p_res)) * 100

    if is_RFE_mode:
        if verbose: print('n_dims:', n_dims_RFE, accuracy)
        return np.mean(accuracy), X_new, X_mask

    return np.mean(accuracy)


def train(dataset_path):
    random.seed(1)
    use_imbalance_dataset = True
    reduces_sample_size = -1

    image_list = prefetch_images(dataset_path)

    if len(image_list) <= 0:
        return

    features = {} # dictionary<family name, list of feature vector>
    for i in range(len(image_list)):
        family = image_list[i][0]
        images = image_list[i][1]
        n_images = len(images)
        if reduces_sample_size > 0:
            n_images = min(reduces_sample_size, n_images)
        min_samples = min(80, n_images)
        max_samples = 120
        num_samples = min(max(n_images, min_samples), max_samples)
        image_idx_list = random.sample(range(n_images), num_samples)
        if use_imbalance_dataset:
            image_idx_list = range(n_images)
        print("fetching...", family)
        sample_count = 0
        for j in image_idx_list:
            image_path = dataset_path + family + '/' + images[j]
            print(image_path, sample_count + 1, "(", j, ")", "out of", len(image_idx_list))
            img = io.imread(image_path)

            # Feature 1: horizontal edge: 1 x 256
            smooth_radius = 3
            hist_size = 64
            # original setting: 9 and 256
            # tuned setting: 3 and 64
            hist = project_h_edge(img, gauss_sigma=smooth_radius, print_img=False, total_blocks=hist_size)

            # resize to desired length. e.g. 256
            hist_arr = np.array(hist).reshape(1, len(hist))
            hist = transform.resize(hist_arr, (1, hist_size), mode='reflect').tolist()[0]

            # Feature 2: Histogram of Gaussian
            hog_block_size = (4, 4)
            hog_buckets = 128
            # original setting: (12, 12) and 1
            # tuned setting: (4, 4) and 128
            hog_feature = extract_HOG(img, blocks=hog_block_size, buckets=hog_buckets).tolist()

            # ---------------------------
            # if not using mean value, the dimension would be 8100
            # ---------------------------

            # Feature 3: Mean intensity of each grid
            grid_block_size = (16, 16)
            # original setting: (3, 3)
            # tuned setting: (16, 16)
            grids = extract_grid_blocks(img, blocks_per_image=grid_block_size)
            mean_vec = means_feature(grids)

            # Feature 4: LBP
            lbp_radius = 2
            # original setting: 2
            # tuned setting: 2
            lbp_hist = extract_lbp_feature(img, radius=lbp_radius)

            # Feature 5: Histogram(Contrast)
            contrast_hist = histogram_feature(img)

            # Feature 6: Media images (flatten)
            median_shape = (64, 64)
            # original setting: (64, 64)
            # tuned setting: (64, 64)
            median_vec = median_feature(img, resize_shape=median_shape)

            feature_vec = hist + hog_feature + mean_vec + lbp_hist + contrast_hist + median_vec

            if family not in features:
                features[family] = list()
            l = features.get(family)
            l.append(feature_vec)
            sample_count += 1

    accu = fitting_scoring(features, verbose=True)

    # Example1: exhaustive search for all accuracies
    # accu = feature_selection_RFE(features)

    # Example2: get reduced feature vectors then apply a quantizer
    # accu, X_new, X_mask = fitting_scoring(features, cv=2, verbose=False, is_RFE_mode=True, n_dims_RFE=10)
    # quantize_feature(X_new)

    print(accu)


def feature_selection_RFE(features):
    """
    Enumerate all accuracies of reducing feature dimension down to 1.
    :param features: ndarray
            MxN matrix with M is number of samples and N is feature dim.
    :return: list
            All accuracies w.r.t. to each step when on reduced features.
    """
    feature_test = features[list(features.keys())[0]]
    n_dims = len(feature_test[0])
    overall_accu = list()
    for dim_picked in range(n_dims, 0, -1):
        accu = fitting_scoring(features, cv=2, verbose=False, is_RFE_mode=True, n_dims_RFE=dim_picked)
        overall_accu.append(accu[0])

    return overall_accu
